# -*- coding: utf-8 -*-
"""IDX_Partners_VIX_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NCUCJxi6fiSO0jich6i-vCtq46yka0er

#**Machine Learning for Credit Risk Analysis - ID/X Partners Project-Based Internship**

A lending company, sought to enhance their risk assessment capabilities by developing a predictive model capable of assessing creditworthiness. The model would leverage a dataset comprising accepted and rejected loan applications to identify patterns and correlations indicative of credit risk. The project aimed to provide a comprehensive solution encompassing model development, evaluation, and visual representation of findings, facilitating informed decision-making for the lending company.

The timely development of this predictive model was crucial for the lending company to mitigate financial losses arising from loan defaults. By accurately identifying high-risk borrowers, the company could implement effective risk management strategies, optimize lending decisions, and improve overall profitability. Additionally, through Exploratory Data Analysis (EDA), we can
uncover patterns and trends within the data that reveal deeper insights into the factors influencing creditworthiness.

**Project for Data Scientist Project-Based Internship by ID/X Partners**

**By Giselle Halim**

### Import Library
"""

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import confusion_matrix

import warnings
warnings.filterwarnings('ignore')

"""### Load Dataset"""

df = pd.read_csv('loan_data_2007_2014.csv')

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 75)

"""### Sneak Peak Data"""

#Looking at the first 5 rows of the dataset
df.head()

#Looking at the last 5 rows of the dataset
df.tail()

#How many rows and columns in the dataset?
df.shape

#General information of the dataset
df.info()

"""### Handling Missing Values"""

#Checking for missing values
df.isnull().sum()

#Deleting unused columns
dropped = ['id', 'member_id', 'Unnamed: 0', 'funded_amnt', 'funded_amnt_inv', 'url', 'desc', 'title', 'zip_code',
           'next_pymnt_d', 'last_pymnt_d', 'mths_since_last_delinq', 'mths_since_last_record', 'recoveries',
           'collection_recovery_fee', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d',
           'mths_since_last_major_derog', 'total_pymnt', 'total_pymnt_inv', 'total_rec_prncp', 'total_rec_int',
           'out_prncp_inv', 'total_rev_hi_lim']

df.drop(columns = dropped, axis=1, inplace=True)

#Deleting all columns with null value above 90%
df = df.loc[:, df.isnull().mean() < .9]

df.isnull().sum()

"""## Data Preprocessing

###Loan Status

To streamline the model's learning process and enhance prediction accuracy, the loan_status variable was categorized into two distinct classes.

* 0: Good (loans with timely payments or specific exemptions)
* 1: Bad (loans with overdue payments exceeding 30 days).

By reducing the number of categories, the model could focus on identifying the most significant factors influencing credit worthiness, minimizing the risk of
overfitting and improving overall predictive performance.

The 'Current' loan status category is excluded from the analysis as these loans are ongoing and cannot be predicted. The 'Late (16-30 days)' category is treated as a good loan since loans are typically not classified as bad until they surpass 30 days overdue.
"""

df.loan_status.value_counts()

good_loan = ['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid', 'In Grace Period', 'Late (16-30 days)']
df['loan_stats'] = np.where(df['loan_status'].isin(good_loan), 0, 1)

df = df.loc[~df['loan_status'].isin(['Current'])].reset_index(drop=True)
df.info()

df.drop('loan_status', axis=1, inplace=True)

"""### Categorical Columns with 1 Unique Value"""

df['policy_code'].unique()

#Policy code only has 1 unique value
df.drop('policy_code', axis=1, inplace=True)

#Application type only has 1 unique value
df['application_type'].unique()

df.drop('application_type', axis=1, inplace=True)

"""### Cleaning Categorical Columns"""

df['emp_length'].unique()

#Cleaning emp_length values
df['emp_length'] = df['emp_length'].str.replace('\+ years', '')
df['emp_length'] = df['emp_length'].str.replace('< 1 year', str(0))
df['emp_length'] = df['emp_length'].str.replace(' years', '')
df['emp_length'] = df['emp_length'].str.replace(' year', '')
df['emp_length'] = df['emp_length'].str.replace('+', '')
df['emp_length'] = df['emp_length'].astype(float)

#Converting emp_title values to uppercase
df['emp_title'] = df['emp_title'].str.upper()

#Cleaning term values
df['term'] = df['term'].str.replace(' months', '')
df['term'] = df['term'].astype(float)

#Looking at values in other columns
cat = df.select_dtypes (include= ['object'])

for col in cat.columns.tolist():
    print(df[col].value_counts()[:20])
    print('\n')

#Dominated by a single value
df.drop('pymnt_plan', axis=1, inplace=True)

"""###Date

####earliest_cr_line
"""

df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'], format='%b-%y')
df['mths_since_earliest_cr_line'] = (pd.to_datetime('2016-12-31') - df['earliest_cr_line']).dt.days // 30
df['mths_since_earliest_cr_line'].head()

df['mths_since_earliest_cr_line'].describe()

df.loc[df['mths_since_earliest_cr_line']<0, 'mths_since_earliest_cr_line'] = df['mths_since_earliest_cr_line'].max()

df.drop(['earliest_cr_line'], axis=1, inplace=True)

"""####issue_d"""

df['issue_d'] = pd.to_datetime(df['issue_d'], format='%b-%y')
df['issue_d'].head()

df['mths_since_issue_d'] = (pd.to_datetime('2016-12-31') - df['issue_d']).dt.days // 30
df['mths_since_issue_d'].head()

df['mths_since_issue_d'].describe()

df.head()

"""#### issue_year"""

#Extract the year from 'issue_d'
df['issue_year'] = pd.to_datetime(df['issue_d']).dt.year

"""## Exploratory Data Analysis"""

#Describing the dataset
df.describe()

# Looking at categorical columns
cat = df.select_dtypes (include= ['object'])

for col in cat.columns.tolist():
    print(df[col].value_counts())
    print('\n')

"""### Loan Status"""

data_plot  = df['loan_stats'].value_counts().to_list()
label_plot = df['loan_stats'].value_counts().index.to_list()

title = 'Loan Status (0 = Good, 1 = Bad)'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot_title = plt.title(title)

plt.show()

df['loan_stats'].value_counts()

# Average numerical columns based on loan status
df.groupby(by = 'loan_stats').mean(numeric_only=True)

"""* The data shows a skewed distribution where significantly more borrowers have good credit than bad
credit, which may lead to biased model predictions if not addressed through techniques like resampling.
* There are 191,091 good loans compared to 50,968 bad loans. The percentage of bad loans to the total
number of loans is approximately 21.06%. This is quite high and a potential red flag under most normal
conditions. It may indicate significant financial stress or a risky lending environment.
* The average loan amount is around $13-14k with an interest rate of 13-15%. Bad loans tend to have a
higher average loan amount than good loans, possibly indicating overborrowing by high-risk borrowers.

### Issue Year vs Loan Status
"""

grouped_data = df.groupby('issue_year')['loan_stats'].value_counts().unstack()

default_rate = grouped_data[1] / (grouped_data[0] + grouped_data[1])
default_rate.name = 'default_rate'

results = pd.concat([grouped_data, default_rate], axis=1)

print(results)

results['default_rate'].plot(kind='bar', figsize=(10, 6))
plt.xlabel('Issue Year')
plt.ylabel('Default Rate')
plt.title('Default Rate by Issue Year (Descending Order)')
plt.show()

"""The high default rate of 26% in 2007 can likely be attributed to the onset of the global financial crisis, which started in that year. The housing market collapse, widespread foreclosures, and tightening credit conditions caused significant financial strain, leading to a spike in defaults. In 2008, the default rate
decreased to 20.7%, which, while still high, reflects the ongoing effects of the crisis as governments and financial institutions implemented emergency measures, including bailouts and loan modifications, to stabilize the economy.

From 2009 to 2012, the default rate continued to decrease to between 13-16%. This decline likely reflects the gradual economic recovery following the recession, as markets stabilized and job growth slowly improved. Additionally, lending practices became stricter, reducing the number of high-risk loans and improving overall loan quality. The Dodd-Frank Act and other post-crisis financial regulations may have played a key role in reducing risky lending, leading to a much healthier credit environment.

However, the rate rose again to 22-25% during 2013-2014. This increase can be attributed to heightened economic uncertainty, driven by the U.S. debt ceiling crisis, global market volatility, and the Federal Reserve's tapering of its quantitative easing program. The political standoff over the debt ceiling created
fears of a government default, while concerns about slower growth in emerging markets and the Eurozone debt crisis intensified market instability. Additionally, the Fed's decision to reduce its bond-buying program led to rising interest rates, further straining borrowers. These combined factors made it difficult for
businesses and borrowers to manage financial pressures, resulting in higher default rates during this period.

### Employee Title + Length
"""

data_plot  = df['emp_title'].value_counts()[:20].to_list()
label_plot = df['emp_title'].value_counts()[:20].index.to_list()

title = 'Top 20 Professions'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot.set_xticklabels(plot.get_xticklabels(), rotation=45, horizontalalignment='right')
plot_title = plt.title(title)
plt.show()

df['emp_title'].value_counts()[:20]

data_plot  = df['emp_length'].value_counts().to_list()
label_plot = df['emp_length'].value_counts().index.to_list()

title = 'Employment Length'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot_title = plt.title(title)

plt.show()

df['emp_length'].value_counts()

top_20_emp_title_0 = df[df['loan_stats'] == 0]['emp_title'].value_counts()[:20]
print("Top 20 'emp_title' for loan_stats 0:\n", top_20_emp_title_0)

top_20_emp_title_1 = df[df['loan_stats'] == 1]['emp_title'].value_counts()[:20]
print("\nTop 20 'emp_title' for loan_stats 1:\n", top_20_emp_title_1)

"""Managers and teachers are among the most frequent borrowers and are more likely to default on loans. This may stem from job-related financial strain, such as underpaid work or responsibilities requiring personal financial contributions (in the case of teachers), or from overextension of credit and debt accumulation due to higher living expenses or lifestyle demands for managers.

The average borrower has been employed for 10 or
more years, but this does not necessarily indicate
lower credit risk. Despite long work histories, other
factors may influence default risk.

### States vs Loan Status
"""

data_plot  = df['addr_state'].value_counts()[:20].to_list()
label_plot = df['addr_state'].value_counts()[:20].index.to_list()

title = 'Top 20 States'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot.set_xticklabels(plot.get_xticklabels(), rotation=45, horizontalalignment='right')
plot_title = plt.title(title)
plt.show()

data_plot  = df['addr_state'].value_counts()[-20:].to_list()
label_plot = df['addr_state'].value_counts()[-20:].index.to_list()

title = 'Least 20 States'

plot       = sns.barplot(x = label_plot, y = data_plot, palette = 'CMRmap')
plot.set_xticklabels(plot.get_xticklabels(), rotation=45, horizontalalignment='right')
plot_title = plt.title(title)
plt.show()

state_bad_loan_ratio = df.groupby('addr_state')['loan_stats'].value_counts(normalize=True).unstack()
state_bad_loan_ratio['bad_loan_ratio'] = state_bad_loan_ratio[1] / (state_bad_loan_ratio[0] + state_bad_loan_ratio[1])

state_bad_loan_ratio = state_bad_loan_ratio.sort_values(by='bad_loan_ratio', ascending=False)

print(state_bad_loan_ratio)

print("\nTop 5 highest bad loan ratio states:")
print(state_bad_loan_ratio['bad_loan_ratio'].head(5))

print("\nTop 5 lowest bad loan ratio states:")
print(state_bad_loan_ratio['bad_loan_ratio'].tail(5))

"""* California (CA) takes on the most debt, which can be attributed to its large population and high cost of living. New York (NY), Texas (TX), and Florida (FL) also have high debt, though at lower levels, reflecting regional economic diversity and varying cost pressures. For instance, NY's financial hub and TX's rapid population growth both contribute to higher levels of borrowing.

* States like Nebraska (NE), Iowa (IA), Idaho (ID), and Maine (ME) report the lowest levels of debt, with fewer than 500 accounts in debt. These states' smaller populations, lower living costs, and more conservative attitudes toward debt contribute to less reliance on credit
* The higher default rates in states like Nebraska (NE), Mississippi (MS), Tennessee (TN), Indiana (IN), and Nevada (NV) likely reflect economic challenges such as lower incomes, higher unemployment, or market instability. Nebraska’s particularly high default ratio of 0.57% may be linked to localized issues like industry downturns or agricultural instability.

* States like New Hampshire (NH), Wyoming (WY), and Washington,
D.C. (DC) have low bad loan rates, indicating stronger financial
stability or conservative borrowing habits. Maine (ME) stands out
with no recorded bad loans, possibly due to a low-risk borrower base or effective lending practices, influenced by smaller populations and fewer risky loans.

### Risk Percentage of Bad Loan
"""

#Risk percentage of bad loan
def risk_percentage(x):
    ratio = (df.groupby(x)['loan_stats']
         .value_counts(normalize=True)
         .mul(100)
         .rename('risk (%)')
         .reset_index())

    sns.lineplot(data=ratio[ratio['loan_stats'] == 1], x=x, y='risk (%)')
    plt.xticks(rotation=45, ha='right')
    plt.title(x)
    plt.show()

columns = ['term', 'initial_list_status', 'verification_status', 'home_ownership', 'acc_now_delinq','grade', 'sub_grade', 'inq_last_6mths', 'collections_12_mths_ex_med', 'emp_length', 'mths_since_issue_d', 'mths_since_earliest_cr_line', 'purpose', 'int_rate']
for cols in columns:
    risk_percentage(cols)

"""* Lower credit grades (e.g., G) are associated with higher default risks. Borrowers in these grades typically have weaker financial standing, as reflected in their credit assessments.
* Longer loan terms, involving more installments, are correlated with a higher likelihood of bad credit. Borrowers may struggle with long-term repayment commitments, especially if financial circumstances change.
* A higher number of delinquent accounts directly increases the likelihood of default. These accounts suggest past financial mismanagement or challenges in meeting financial obligations.
* An increased number of credit checks within the last 6 months is a strong predictor of higher default risk. This behavior often indicates financial distress or a borrower seeking multiple credit lines, potentially signaling overextension.
* Borrowers who own or rent a home show a higher risk for bad loans. This is an interesting finding, especially as borrowers without homes tend to exhibit a lower risk.
* Loans with terms exceeding 100 months show a trend of increasing risk over time. Long-term loans often face more uncertainty, as borrowers' financial circumstances can change significantly over extended periods.
* Loans used for small businesses are the most at risk. Small businesses typically face greater financial volatility. They are more susceptible to market shifts, economic downturns, and cash flow problems, making loans for small businesses inherently riskier.
* Borrowers with a longer borrowing history often demonstrate reliable repayment behavior and build a positive credit track record. However, this isn't always a perfect measure, as other factors like changes in income or economic conditions can still affect risk.
* Debt in the whole (W) market tends to carry slightly higher risk than debt in the fractional (F) market. This may be due to different investor profiles or underwriting standards across these markets.
* Borrowers with more than 2 bills per year (excluding medical bills) face a higher risk of default. This could indicate difficulty in managing day-to-day expenses, leading to financial strain.

* There is a general trend that higher interest rates correlate with increased default risk. Borrowers paying higher rates may already be considered higher risk, thus having a greater likelihood of defaulting.

### Loan Status in Depth
"""

# Loan status vs grade
pd.crosstab(index=df['loan_stats'], columns=df['grade'])

# Loan status vs sub-grade
pd.crosstab(index=df['loan_stats'], columns=df['sub_grade'])

# Loan status vs months since issue date
pd.crosstab(index=df['loan_stats'], columns=df['mths_since_issue_d'])

# Loan status vs months since earliest credit
pd.crosstab(index=df['loan_stats'], columns=df['mths_since_earliest_cr_line'])

# Loan status vs home ownership
pd.crosstab(index=df['loan_stats'], columns=df['home_ownership'])

# Loan status vs income verification (amount and source verification)
pd.crosstab(index=df['loan_stats'], columns=df['verification_status'])

# Loan status, loan amount, and grade
pd.pivot_table(df, values='loan_amnt', index='loan_stats', columns='grade', aggfunc=np.mean)

# Loan status, annual income, and grade
pd.pivot_table(df, values='annual_inc', index='loan_stats', columns='grade', aggfunc=np.mean)

# Loan status, revolving balance, and grade
pd.pivot_table(df, values='revol_bal', index='loan_stats', columns='grade', aggfunc=np.mean)

# Loan amount vs home ownership
pd.pivot_table(df, values='loan_amnt', index='loan_stats', columns='home_ownership', aggfunc=np.mean)

# Loan status vs purpose
pd.crosstab(index=df['loan_stats'], columns=df['purpose'])

"""* Borrowers with mortgages or rental statuses are more likely to take out loans. Mortgage holders may be using debt to manage high financial obligations, while renters might face less stable financial situations.

* Individuals with lower credit grades often carry higher revolving balances, indicating a greater reliance on loans and an increased likelihood of carrying unpaid debt month to month. This behavior suggests higher financial risk, as larger revolving balances can lead to greater interest accrual and potential
difficulty in managing debt.
* Borrowers in lower credit grades tend to take on larger debts, further increasing their risk of default. This trend suggests that lower creditworthiness does not always curb loan sizes, potentially exacerbating risk.

* Borrowers with bad loans typically have lower average annual incomes, reflecting a strong link between income and credit risk. Lower-income individuals may struggle more to meet debt obligations, leading to higher default rates.

### Visualization
"""

column_name_list_num = ['loan_amnt', 'term', 'int_rate', 'installment', 'emp_length', 'annual_inc', 'dti', 'delinq_2yrs']
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Histplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.histplot(x=var, hue = 'loan_stats', data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title(var + " " + "by loan status")
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Barplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.boxplot(x=var, data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title("Boxplot of" + " " + var)
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

column_name_list_num = ['inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', 'total_acc', 'collections_12_mths_ex_med']
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Histplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.histplot(x=var, hue = 'loan_stats', data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title(var + " " + "by loan status")
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Barplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.boxplot(x=var, data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title("Boxplot of" + " " + var)
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

column_name_list_num = ['last_pymnt_amnt', 'revol_bal', 'acc_now_delinq', 'tot_coll_amt', 'tot_cur_bal', 'mths_since_earliest_cr_line', 'mths_since_issue_d']
num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Histplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.histplot(x=var, hue = 'loan_stats', data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title(var + " " + "by loan status")
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

num_cols = len(column_name_list_num)
num_rows = (num_cols + 2) // 3
fig, axs = plt.subplots(nrows=num_rows, ncols=3, figsize=(15,5*num_rows))
axs = axs.flatten()

#Barplot for each variables
for i, var in enumerate (column_name_list_num):
  sns.boxplot(x=var, data=df, palette = 'CMRmap', ax=axs[i])
  axs[i].set_title("Boxplot of" + " " + var)
  axs[i].tick_params(axis='x', rotation=90)

#Removes extra empty subplots
if num_cols < len(axs):
  for i in range(num_cols, len(axs)):
    fig.delaxes(axs[i])

fig.tight_layout()
plt.show()

"""##Missing Data Imputation"""

#Removing unnecessary data
dropped = ['emp_length', 'emp_title', 'mths_since_issue_d', 'sub_grade', 'issue_d', 'issue_year']
df.drop(columns = dropped, axis=1, inplace=True)

"""emp_title and emp_length are excluded due to too many unique values and limited relevance to credit risk. Sub-grade is dropped in favor of grade, which simplifies the model by reducing category complexity. Also, columns like mths_since_issue_d and issue_year are removed because they contain data unavailable at the time of loan approval."""

df.isnull().sum()

# Drop null values
df.dropna(inplace=True)

# Changing data type because the value is in whole number, not decimals
df['open_acc'] = df['open_acc'].astype(int)
df['pub_rec'] = df['pub_rec'].astype(int)
df['total_acc'] = df['total_acc'].astype(int)
df['acc_now_delinq'] = df['acc_now_delinq'].astype(int)

df.info()

"""## Feature Correlation"""

# Select only int and float columns
df_corr = df.select_dtypes(include=['int', 'float'])

# Calculate the correlation matrix
corr_matrix = df_corr.corr()

# Create a heatmap of the correlation matrix
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Find columns with correlation above 0.7
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.7)]
print(to_drop)

# Drop the highly correlated columns
df.drop(to_drop, axis=1, inplace=True)

"""## Outliers"""

def subset_by_iqr(df, column):
    """Remove outliers from a dataframe by column, including optional
       whiskers, removing rows for which the column value are
       less than Q1-1.5IQR or greater than Q3+1.5IQR.
    Args:
        df (`:obj:pd.DataFrame`): A pandas dataframe to subset
        column (str): Name of the column to calculate the subset from.
        whisker_width (float): Optional, loosen the IQR filter by a
                               factor of `whisker_width` * IQR.
    Returns:
        (`:obj:pd.DataFrame`): Filtered dataframe
    """
    whisker_width=1.5

    # Calculate Q1, Q2 and IQR
    q1 = df[column].quantile(0.25)
    q3 = df[column].quantile(0.75)
    iqr = q3 - q1

    # Apply filter with respect to IQR, including optional whiskers
    filter = (df[column] >= q1 - whisker_width*iqr) & (df[column] <= q3 + whisker_width*iqr)
    return df.loc[filter].reset_index(drop=True)

outlier = ['annual_inc', 'last_pymnt_amnt','tot_coll_amt','tot_cur_bal']

print(f'Count of rows before removing outlier: {len(df)}')
for i in outlier:
  df = subset_by_iqr(df, i)
print(f'Count of rows after removing outlier: {len(df)}')

"""## One-Hot Encoding"""

# Categorical data
cat = df.select_dtypes (include= ['object'])
cat

# One-hot encoding for categorical features
cat_features = df.select_dtypes(include=['object']).columns
df = pd.get_dummies(df, columns=cat_features, drop_first=True)

"""## Splitting the dataset into the Training set and Test set"""

#Defining x and y
x = df.drop(columns=['loan_stats'], axis = 1)
y = df['loan_stats']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)

"""##Balancing data"""

from imblearn.over_sampling import SMOTE

smote = SMOTE(random_state=42)
x_train, y_train = smote.fit_resample(x_train, y_train)

"""## Modelling"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(class_weight={0: 1, 1: 10})

#Training the model
rf.fit(x_train, y_train)

#Predict testing set
y_pred = rf.predict(x_test)

print('Training-set accuracy score:', rf.score(x_train, y_train))
print('Test-set accuracy score:', rf.score(x_test, y_test))

plt.figure(figsize=(6,6))
sns.heatmap(confusion_matrix(y_test,y_pred), annot=True, fmt='d', cmap='Purples')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix for Random Forest')
plt.show()

#Check model performance using classification_report
print(classification_report(y_test, y_pred))

#Check model performance using auc score
roc_auc_score(y_test, y_pred)*100

"""The model performs well in detecting bad loans, even with imbalanced data. The model's AUC score reaches 89%, indicating strong predictive performance and reliability in identifying high risk loans."""

importances = rf.feature_importances_
feature_importances = pd.DataFrame({'feature': x_train.columns, 'importance': importances})
feature_importances = feature_importances.sort_values('importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 20))
sns.barplot(x='importance', y='feature', data=feature_importances)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest Model')
plt.show()

"""The chart above illustrates the feature importance, highlighting how each factor contributes to the Random Forest model's ability to predict credit risk. The most significant feature is last_pymnt_amnt, indicating that the last payment amount plays a key role in determining credit risk. Int_rate (interest rate) is also a critical feature. It's important to note that the chart excludes categorical features, which have already been one-hot encoded."""